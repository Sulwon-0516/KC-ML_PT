# ResNet

연구의 배경

1. Conv-NET이 image classification의 패러다임을 바꿈
    1. 16~30 층의 deep models에서 좋은 결과를 얻을 것을 보아 deep model을 유리함.
2. 하지만 layer를 쌓을수록 vanishing/exploding gradients 문제가 발생함.
    1. 10층 가량은는 normalized initialization과 intermediate normalization으로 수렴은 함
3. 하지만 degradation 문제가 발생
    1. accuracy saturation + degradation
    2. overfitting때문은 아님. training error도 느는 것을 확인 가능.

        [https://lh4.googleusercontent.com/FlRhUioe5en3mUq0iLM7WrswEGvoV8SjLXmJtN288JREicOBlVlpAoCXPkcECTTPG4nD27ZT1oVBEczv0dXrN7hU62_p2uYSAi240ZqnqYU85OB85-HODX7OsILYXCxGwS7_y0sT](https://lh4.googleusercontent.com/FlRhUioe5en3mUq0iLM7WrswEGvoV8SjLXmJtN288JREicOBlVlpAoCXPkcECTTPG4nD27ZT1oVBEczv0dXrN7hU62_p2uYSAi240ZqnqYU85OB85-HODX7OsILYXCxGwS7_y0sT)

4. 그러면 learned shallower layer에 identity mapping을 쌓아서 학습을 시켜보자.
    1. 하지만 shallow model에서보다 좋은 결과를 얻지 못함.
5. deep residual learning framework로 이 degradation을 해결하고자함.
    1. F(x) = H(x) - x
    2. shortcut connection이란 skipping path를 만드는 것
    3. 이는 optimize라기 쉽고, depth가 늘려 좋은 accuracy를 얻을 수 있음.
6. CIFAR-10으론 100~1000 layers를 보았고, imageNet으론 152 layers로 좋은 결과를 얻음
    1. 152 layers임에도 VGG보다 복잡도가 적음.
7. 우리는 ILSVRC 2015를 이거로 1등함 (3.58% error)
8. 유사 연구와의 차별성 + 배경
    1. Residual Representations
        1. dictionary에 대한 residual vector로 구성된 VLAD
        2. Fisher Vector : Prob. version of VLAD
        3. 이 두가지는 이미지 검색과 분류에서 orginal vector 쓰는 것보다 우수?
        4. PDE를 풀때 Multigrid method가 주로 쓰이는데 residual vetor를 써서 solver가 빨리 수렴는 것을 볼 수 있었음
        5. 이런 관점에서 잔차를 도입함.
    2. Shortcut Connection
        1. Google Net의 inception layer에 있는 1x1 conv shortcut 언급
        2. highway networks가 shortcut connection을 사용해서 유사함.
        3. 하지만, 위의 연구는 short-cut에도 learnable parameter가 들어가서 shortcut이 close 되는 문제가 존재함.
        4. 우린 이런 non-residual function이 되는 것이 없어서 다름
        5. 또한, 이들은 100 layers 와 같은 시도에서 실패했으니 우리는 다른 연구임.
9. Deep Residual Learning
    1. Residual learning
        1. Hypothesis H(x)가 학습이 가능(approximate)하면 H(x)-x도 가능할 것이다.
        2. 그러면 F(x)=H(x)-x를 학습시키자. 그러면 H(x) = F(x)+x가 된다.
        3. 둘다 학습이 되어야 하지만, 학습의 난이도가 차이가 클 것이다. 
        4. 이 아이디어는, added layer를 identity layer로 동작하게 해서 degradation을 해결하자는 것에서 나옴.
        5. identity mapping이 적당하면, solver가 nonlinear layer를 zero로 만들어서 identity mapping으로 구성할 것을 기대함. 
        6. 실제로 identity mapping이 optimal하진 않지만, 이런 방법은 precondition에 도움을 줌.
        7. 

            [https://lh5.googleusercontent.com/H0hsbBhSGXuPfmuohz93qEfiJ_9W7EZsRTegLAF5adP0oSRux6t-S63CLN5_Nni2ZdK7OBGWuRqvy0PiyZkUck-dFQUtjx7pFSDRyUrH1RT_TSrJAccWeThM1_2Iyh3q-79RJjLR](https://lh5.googleusercontent.com/H0hsbBhSGXuPfmuohz93qEfiJ_9W7EZsRTegLAF5adP0oSRux6t-S63CLN5_Nni2ZdK7OBGWuRqvy0PiyZkUck-dFQUtjx7pFSDRyUrH1RT_TSrJAccWeThM1_2Iyh3q-79RJjLR)

        8. 이를 언급하면서, learned residual function이 작은 responses를 보인다면서 identity mapping 이 유효한 preconditioning을 만들었다고 함.
        9. identity mapping이 항등사상이라 나오는데, 개념을 완전히 이해하진 못하겠음. -> 결국 short cut으로 일부 레이어를 skip하는 형식으로 학습이 가능하고, 실제 conv layer가 response가 작은 것을 보아, 기대한대로 작동한 것 같다고 주장하는 거라 받아들임.
    2. Identity Mapping by Shortcuts
        1. 2 레이어를 한 세트로 residual learning을 적용함.
        2. , w는 learnable weight를 의미

            [https://lh3.googleusercontent.com/H1KCZc_OCQr9WaOWI5jRINgZwns3ukWR1zv4qSi22CG6_uhsFt-HMpO6DZoUzRo36jsgQikEW5DbmOgZ5-WHTKyoNRs7xQ8oBrtaYI9u1nYuPAO9KL8Njgf7FYTXuogeGLbc0Zoq](https://lh3.googleusercontent.com/H1KCZc_OCQr9WaOWI5jRINgZwns3ukWR1zv4qSi22CG6_uhsFt-HMpO6DZoUzRo36jsgQikEW5DbmOgZ5-WHTKyoNRs7xQ8oBrtaYI9u1nYuPAO9KL8Njgf7FYTXuogeGLbc0Zoq)

        3. addition 뒤에 ReLU로 second nonlinearity를 가함.
        4. plain network에 비해 복잡도가 바뀌지 않으니, 공평하게 비교할 수 있음.
        5. F(x)와 x의 차원이 같아야함. 그렇지 않으면, x에다가 linear projection을 달아줄 수 있음.
            1. 이미 Identity mapping으로 충분함을 보았기에, 이 linear projection은 차원 조정을 위해서만 쓰임.
        6. F(x)는 여기서 2 layer로 쓰였지만, 더 커도 됨. Single layer이면 linear layer가 되니 X.
        7. F(x)는 FCL이든 CONV든 상관 없음.
    3. Network Architectures
        1. ImageNet에서 토론을 위해 2가지 모델 준비함.
        2. Plain Network(비교군)
            1. AlexNet을 확장한 VGG에서 영향을 받음.
            2. feature map size가 같도록 하기 위해서, 같은 갯수의 필터를 쓰고, 차원이 줄면 필터 갯수를 늘려줌. (레이어 당 시간 소모를 같게 하려고)
            3. 처음에 stride 2로 downsampling을 하고서 넣어줌
            4. 총 34layers.
            5. VGG의 FC4096이 빠져서 18%만의 연산이 요구됨
        3. Residual Network
            1. 위의 plain Network에다가 shortcut들을 달아버림.
            2. 차원이 바뀌었을 때는, zero padding을 하거나, 1x1 conv로 해결을 함.
    4. Implementation
        1. 이미지는 256x480으로 resize하고 여기서 224x224를 random하게 잘라서 사용. (horiziontal flip등 augmentation을 줌)
        2. standard color augmentation 도 쓰임.
        3. BN(batch normalization)을 CONV <> ReLU사이에 적용함.
        4. initialization은 Initialization of Filter Weights for Rectifiers 을 썼다는데, [https://arxiv.org/abs/1502.01852](https://arxiv.org/abs/1502.01852) 위 논문에 처음 제시된 방법으로, Xavier랑 다르게 deep한 모델에 stall을 주지 않는다네. (rectifier nonlinearity를 썼다는데, 논문을 읽어봐야 이해가 될듯)
        5. SGD에다가 mini-batch 256으로 학습
        6. Learning rate을 0.1로 잡고, error가 지속되면(??) 10으로 나눠줌.
        7. 600k iteration까지 학습시킴.
        8. weight decay : 0.0001에 momentum : 0.9 (아니 SGD-Momentum이네)
        9. dropout은 안 썻음. 
        10. Standard 10-crop testing을 썼다면서 ImageNET 논문을 레퍼런스로 걸어두는데 설명이 없어서 찾아봄

[https://lh4.googleusercontent.com/IlieVQfcJ6F3tXJfSDt3pEvG16IkyLYZlKuavrT_SMcO_-H8anq5_1nYRCaaA_1ZQog0yavyicvUecRufxX9weV9cQeipyn387DyiQ4otfizHl15YbZxs2ndxsooRbiIbb8wfjnT](https://lh4.googleusercontent.com/IlieVQfcJ6F3tXJfSDt3pEvG16IkyLYZlKuavrT_SMcO_-H8anq5_1nYRCaaA_1ZQog0yavyicvUecRufxX9weV9cQeipyn387DyiQ4otfizHl15YbZxs2ndxsooRbiIbb8wfjnT)

위 그림처럼 4 모서리 + 중심을 crop하고 horizontal flip해서 총 10장으로 testing을 하는 것. [http://cs231n.stanford.edu/reports/2016/pdfs/411_Report.pdf](http://cs231n.stanford.edu/reports/2016/pdfs/411_Report.pdf)

1. fully convolutional from 을 쓰고, 여러 scale에서의 점수를 평균을 냈다고 함.
- > 레퍼런스가 있는데 fully convolutional form을 testing에 쓴다는게 뭔지 이해가 안감. + input size가 다양한 것을 어떻게 넣은지도 이해는 안감.

(끝에 FC1000으로 이을때 문제가 생기지 않나?)

<구조 설명>

ImageNet classifier에서 쓰인 구조들임.

[https://lh3.googleusercontent.com/vUulhXfrCroPAWF-rv30Qbovq59WSvRTZBXB66pOiCg38cm1JhWcpSXpwKmQ_oyG8oUK0zK_jpYP8ykXXdKPutsXZLg5cm1AjWdDoW6FUbPZ2cUJAhGx2yyoXpFgn2II2TmlIpPY](https://lh3.googleusercontent.com/vUulhXfrCroPAWF-rv30Qbovq59WSvRTZBXB66pOiCg38cm1JhWcpSXpwKmQ_oyG8oUK0zK_jpYP8ykXXdKPutsXZLg5cm1AjWdDoW6FUbPZ2cUJAhGx2yyoXpFgn2II2TmlIpPY)

1. Experiments
    1. ImageNet Classifier
        1. 평가를 위해서 ImageNet 2012에 쓰인 데이터셋을 씀.
        2. 1.28m images와 1000 classes, 50k validation images와 100k test images에 대해 top-1, top-5 error rate를 얻음.
        3. Plain Networks
            1. 18 layers와 34 layers를 테스트함.
            2. 34 layers에서 validation error가 더 큼.
            3. degradation이 일어나고 있음. -> 하지만 이 문제는 vanishing에 의한 것이 아님. (BN layer가 들어간 만큼)> 이는 solver가 더 좋은 accuracy를 얻을 수 있지만, convergence rate이 낮아서 생긴 문제로 보임.(추후 연구가 필요하다고 함…)
        4. Residual Networks
            1. 처음엔 increasing dimension을 위해 zero padding을 쓰고 모든 shortcut : identity mapping을 함.즉, plain network와 parameter 개수 차이가 없음
            2. 34 layers가 18 layers에 비해 좋은 결과를 보임. (얇은게 train)

[https://lh5.googleusercontent.com/L3jwU5ft-_Iq9NMiVtXtdh6ZZyCEQ7iTNyvkvTumIosraP5gGR07LlmQwltb8p4Bx5kB_CWhMqVKREzPkprSNcRa82AQHKiV6Di8chmRty4QpJm1kwP9W_KMN0_PVKEcHVW8QnXA](https://lh5.googleusercontent.com/L3jwU5ft-_Iq9NMiVtXtdh6ZZyCEQ7iTNyvkvTumIosraP5gGR07LlmQwltb8p4Bx5kB_CWhMqVKREzPkprSNcRa82AQHKiV6Di8chmRty4QpJm1kwP9W_KMN0_PVKEcHVW8QnXA)

1. training error가 더 낮은 모습을 통해서, generalize할 여지가 있음.
2. 즉, degradation 문제는 해결되었고, 이젠 depth를 늘려 accuracy를 높이고자 함.
3. 또한 plain net대비 3.5%나 top-1 error가 감소했음.
4. plain이나 ResNet이나 둘다 나름 정확하나, converge 속도에 있어서 그래프를 보면 차이가 있음 알 수 있음.
5. Identity Vs. Projection Shortcuts
    1. 앞서서는 그냥 identity shortcuts를 썼지만, projection shortcut 'Wx'는 어떨까?
        1. zero-padding shortcuts로 차원 증가, 나머지는 identity
        2. projection shortcuts로 차원 증가, 나머지는 identity
        3. 모든 shortcuts들이 projection
    2. 위의 3가지 모드 plain대비 우수하나 B>A임. (A의 zeropadding이 non-residual learning을 하게 만들어서 그럴듯)
    3. C가 B보다 좋기는 한데, 큰 차이가 없는 것에서 (top-1에서 1%, top-5에서 0.3% 차이) 그냥 A를 계속 쓰기로 했음.> bottleneck block + 시간/메모리 절약

(근데 PReLU보다 성능 안나왔으면 ISLVRC이기려고 썼을 듯)

1. Deeper Bottleneck Architectures
    1. 기존의 3x3 2개를 하나의 cell로 쓰던 방식에서 1x1 2개로 차원을 바꾸고, 3x3을 비교적 작은 입출력 차원으로 구성하는 bottleneck block을 사용함 (1x1 64 -> 3x3 64 -> 1x1 256)
    2. 이는 time complexity에선 비슷하나 depth를 늘려줌
    3. 이때 projection shortcut을 쓰면 시간이 2배 걸리니, identity shortcut을 씀.
2. 50-layer ResNet
    1. 34 layers의 block을 bottleneck으로 대체하면 50(1)-layer가 된다.
    2. B option을 써서 차원을 늘렸음.
    3. 3.8b FLOPs
3. 101-layer & 152-layer
    1. depth는 늘었으나, 152-layer ResNet(11.3b FLOPs)에서도 VGG-16/19(15.3/19.6b FLOPs)에 비해서 complexity는 낮다.
    2. degradation problem은 발생하지 않았고, 확연한 성능향상을 확인함.

[https://lh5.googleusercontent.com/JLZUa1nRt-DW0TC_2HgO7EBmhyjq5yI4jGZWCuq5nD1mWtHnAWiMKzjJmvhzYFgPxcUJ9h_SAZh3FXcvG14MCJB-dF-RdkRNdS5_efakP8pOFTjkW_9ps8RpdZfRcouhEIhQMjdb](https://lh5.googleusercontent.com/JLZUa1nRt-DW0TC_2HgO7EBmhyjq5yI4jGZWCuq5nD1mWtHnAWiMKzjJmvhzYFgPxcUJ9h_SAZh3FXcvG14MCJB-dF-RdkRNdS5_efakP8pOFTjkW_9ps8RpdZfRcouhEIhQMjdb)

1. Comparisons with State-of-the-art Methods
    1. 6개의 다른 depth를 가진 ResNet을 (152는 2번) ensemble해서 top-5 error 3.57%를 얻어서 ILSVRC를 이김.
    2. 152-layer ResNet은 4.49%의 top 5-error를 보임.
2. CIFAR-10 and Analysis
    1. 50k training images and 10k test images with 10 classes
    2. 매우 깊은 network에서의 특징을 분석하고자 함.
    3. 입력은 32x32이미지 (per-pixel mean subtracted)
    4. 처음에 3x3 conv를 두고 6n개의 3x3 convolutions를 쌓았음.
    5. feature map은 32x32, 16x16, 8x8로 각각 2n개의 layer를 가짐 이때 각 filter의 갯수는 16,32,64임.
    6. 마지막으로 FCL10이랑 softmax가 존재함. 즉 6n+2의 depth를 가짐.
    7. shortcut은 2개마다 하나씩 묶어서 3n개의 shortcuts이 존재함. (A씀) 
    8. weight decay, momentum 다 똑같고, initalization, BN 등등 세팅 다 동일함.
    9. mini batch는 128로 줄었고, 2개의 GPU에 나눠 돌렸음.
    10. LR은 0.1에서 시작, 32k, 48k에서 각각 0.1배 했고, 64k에서 멈춤.
    11. 4pixels가 각 side마다 padded되어서 crop을 통해 augmentation을 했음. 
    12. n : 3, 5, 7, 9일때 결과plain : depth가 오름에 따른 degradation문제가 드러남. (ImageNet과 같음)ResNets: optimization을 해내어서 accuracy gain이 증가함 (위와 같은 경향)
    13. n =18 로 110 layers와 n=200 으로 1202 layers를 보자. 110 layers에서는 0.01로 training error가 0.8이하로 내려갈때까지 warm up을 해주고 0.1로 돌아갔음.(대충 400회때)대강 6.43%로 다른 deep thin networks (FinNet, Highway)보다 우수함.1202 layers에서도 optimization에는 문제가 없었음. 허나 7.93%의 test error로 최고는 아님. (overfitting으로 여겨짐)> drop out이나 max out같은 regularization을 더하면 개선될 듯
    14. BN <> ReLU 사이의 STD를 통해서 response를 보면 plain에 비해서 ResNet은 0에 가까운 경향을 보임. 이는 기대한 이론과 동일한 경향성임.또, 깊을수록 response가 적은 경향을 보임.
3. Object Detection on PASCAL and MS COCO
    1. Faster R-CNN을 detection method로 설정하고 VGG16을 ResNet 101로 대체를 함.> COCO에서 standard metric 6% 증가 (28%의 향상)